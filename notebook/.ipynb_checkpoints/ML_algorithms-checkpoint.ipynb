{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 协方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "协方差用于衡量两个变量的总体误差（方差是协方差的一种特殊情况，即两个变量相同，用于表示一个变量的总体误差），如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。如果协方差为0，则这两个随机变量互不相关。（如果$X_1$与$X_2$是统计独立的，那么二者之间的协方差就是0，因为两个独立的随机变量满足$\\mathbb{E}[X_1X_2]=\\mathbb{E}[X_1]\\mathbb{E}[X_2]$ 。\n",
    "但是，反过来并不成立。即如果$X_1$与$X_2$的协方差为0，二者并不一定是统计独立的。） 独立满足p(X1)\\*p(X2)=p(X1,X2)\n",
    "\n",
    "- 二维随机变量$X_1$和$X_2$的协方差定义为： \n",
    "$$\\begin{align}Cov(X_1,X_2) \n",
    "&=\\mathbb{E}[(X_1-\\mathbb{E}[X_1])(X_2-\\mathbb{E}[X_2])] \\tag{1}\\\\\n",
    "&=\\mathbb{E}[X_1X_2]-\\mathbb{E}[X_1]\\mathbb{E}[X_2] \\tag{2}\\end{align}$$\n",
    "\n",
    "\n",
    "- 推广到高维度随机变量，设$X=(X_1,X_2...X_m)^T$为$m$维__随机变量__，$X_i$为第$i$个随机变量(特征)，称矩阵：\n",
    "$$\\Sigma=(c_{ij})_{m\\times m}=\\left[\\begin{matrix}\n",
    "c_{11}      & c_{12}      & \\cdots & c_{1m}      \\\\\n",
    "c_{21}      & c_{22}      & \\cdots & c_{2m}      \\\\\n",
    "\\vdots      & \\vdots      & \\ddots & \\vdots      \\\\\n",
    "c_{m1}      & c_{m2}      & \\cdots & c_{mm}      \\\\\n",
    "\\end{matrix}\\right]\\tag{3}$$\n",
    "为$m$维随机变量$X$的协方差矩阵，也记为$D(X)$，其中：$$c_{ij}=Cov(X_i,X_j),i,j=1,2,...m\\tag{4}$$ 若$\\mathbb{E}[c_{ij}]=0$，则表示$i,j$维度互不相关。\n",
    "\n",
    "\n",
    "- 给定样本集，估计样本各特征维度的协方差矩阵：样本集$\\{x^{(i)};i=1,2...n\\}$的各维度特征为不同的随机变量，即$~x^{(i)}\\in~R^m$,$~x^{(i)}_1,~x^{(i)}_2...x^{(i)}_m~$分别表示的不同维度的随机变量，且假定各维度已零均值化，根据(1)和(3)该数据集关于各特征的协方差矩阵可表示为：   \n",
    "$$\\Sigma=\\mathbb{E}\\left[\\begin{matrix}\n",
    "x_1^{(i)}x_1^{(i)} & x_1^{(i)}x_2^{(i)} &...& x_1^{(i)}x_m^{(i)}\\\\\n",
    "x_2^{(i)}x_1^{(i)} & x_2^{(i)}x_2^{(i)} &...& x_2^{(i)}x_m^{(i)}\\\\\n",
    "...                &...                 &...&...\\\\\n",
    "x_m^{(i)}x_1^{(i)} & x_m^{(i)}x_2^{(i)} &...& x_m^{(i)}x_m^{(i)}\n",
    "\\end{matrix}\\right]=\\mathbb{E}[x^{(i)}x^{(i)^T}]=\\frac{1}{n-1}\\sum\\limits_{i=1}^n(x^{(i)}x^{(i)^T})\\tag{5}$$  \n",
    "Dividing by $~n-1~$instead of $n$ is a typical way to correct for the bias introduced by using the sample mean instead of the true population mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 向量求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标量对向量，其中 $X^T=(x_1,x_2...x_n)$, $A^T=(a_1,a_2...a_n)$。(python矩阵的shape:$n\\times m$，$n$是样本数量)\n",
    "\n",
    "- 普通标量对向量:\n",
    "$$\\frac{\\partial~y}{\\partial~X}=\\left[\\begin{matrix}\n",
    "\\frac{\\partial~y}{\\partial~x_1}\\\\\n",
    "\\frac{\\partial~y}{\\partial~x_2}\\\\\n",
    "              ...              \\\\\n",
    "\\frac{\\partial~y}{\\partial~x_n}\\\\\n",
    "\\end{matrix}\\right]\\tag{6}$$\n",
    "\n",
    "\n",
    "- 函数值对向量:\n",
    "$$\\begin{align}f(X)\n",
    "&=(A^TX)^2=X^TAA^TX\\\\\n",
    "&=(x_1a_1+x_2a_2+...x_na_n)(x_1a_1+x_2a_2+...x_na_n)\\\\\n",
    "\\end{align}\\tag{7}\\\\$$\n",
    "$\\\\$\n",
    "$$\\frac{f(X)}{X}=\\left[\\begin{matrix}\n",
    "\\frac{\\partial~f(X)}{\\partial~x_1}\\\\\n",
    "\\frac{\\partial~f(X)}{\\partial~x_2}\\\\\n",
    "                ...               \\\\\n",
    "\\frac{\\partial~f(X)}{\\partial~x_n}\\\\\n",
    "\\end{matrix}\\right]\n",
    "=\\left[\\begin{matrix}\n",
    "2a_1(a_1x_1+a_2x_2+...+a_nx_n)\\\\\n",
    "2a_2(a_1x_1+a_2x_2+...+a_nx_n)\\\\\n",
    "            ...               \\\\\n",
    "2a_n(a_1x_1+a_2x_2+...+a_nx_n)\\\\\n",
    "\\end{matrix}\\right]\n",
    "=2\\left[\\begin{matrix}a_1\\\\a_2\\\\...\\\\a_n\\end{matrix}\\right][a_1,a_2...a_n]\\left[\\begin{matrix}x_1\\\\x_2\\\\...\\\\x_n\n",
    "\\end{matrix}\\right]=2AA^TX\\tag{8}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带求和符(矩阵)的运算规则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设$x,y$为同维度列向量：\n",
    "$$\n",
    "\\sum\\limits_{i=1}^mx_ix_i^T\\sum\\limits_{j=1}^ny_jy_j^T=\\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^nx_ix_i^Ty_jy_j^T\\tag{9}\n",
    "$$\n",
    "\n",
    "\n",
    "- 矩阵形式表示：\n",
    "\n",
    "    - $i$表示矢量的第$i$个元素:$$\\sum\\limits_{j=1}^na_jK_{ij}=(Ka)_i$$\n",
    "    \n",
    "    - 以二维为例：$$\\overline{K}_{ij}=K_{ij}-\\frac{1}{n}\\sum\\limits_{l=1}^nK_{lj}-\\frac{1}{n}\\sum\\limits_{k=1}^nK_{ik}+\\frac{1}{n^2}\\sum\\limits_{l=1}^n\\sum\\limits_{k=1}^nK_{lk}$$\n",
    "    \n",
    "$$矩阵形式：\\overline{K}=K-\\left[\\begin{matrix}\n",
    "1/n & 1/n \\\\\n",
    "1/n & 1/n \\\\\n",
    "\\end{matrix}\\right]K-K\\left[\\begin{matrix}\n",
    "1/n & 1/n \\\\\n",
    "1/n & 1/n \\\\\n",
    "\\end{matrix}\\right]+\\left[\\begin{matrix}\n",
    "1/n & 1/n \\\\\n",
    "1/n & 1/n \\\\\n",
    "\\end{matrix}\\right]K\\left[\\begin{matrix}\n",
    "1/n & 1/n \\\\\n",
    "1/n & 1/n \\\\\n",
    "\\end{matrix}\\right]$$\n",
    "\n",
    "$$\\left[\\begin{matrix}\n",
    "\\overline{k}_{11} & \\overline{k}_{12} \\\\\n",
    "\\overline{k}_{21} & \\overline{k}_{22} \\\\\n",
    "\\end{matrix}\\right]=\\left[\\begin{matrix}\n",
    "{k}_{11} & {k}_{12} \\\\\n",
    "{k}_{21} & {k}_{22} \\\\\n",
    "\\end{matrix}\\right]-\\frac{1}{2}\\left[\\begin{matrix}\n",
    "{k}_{11}+k_{21} & {k}_{12}+k_{22} \\\\\n",
    "{k}_{11}+k_{21} & k_{12}+{k}_{22}  \\\\\n",
    "\\end{matrix}\\right]-\\frac{1}{2}\\left[\\begin{matrix}\n",
    "{k}_{11}+k_{12} & {k}_{11}+k_{12} \\\\\n",
    "{k}_{21}+k_{22} & k_{21}+{k}_{22} \\\\\n",
    "\\end{matrix}\\right]+\\frac{1}{2\\times 2}\\left[\\begin{matrix}\n",
    "{k}_{11}+k_{12}+k_{21}+k_{22} & {k}_{11}+k_{12}+k_{21}+k_{22} \\\\\n",
    "{k}_{11}+k_{12}+k_{21}+k_{22} & {k}_{11}+k_{12}+k_{21}+k_{22} \\\\\n",
    "\\end{matrix}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设列向量$~x^{(i)},x^{(j)}\\in R^m$，两向量的核函数可以表示为向量在高维空间的内积：$$K(x^{(i)},x^{(j)})=\\phi(x^{(i)})^T\\phi(x^{(j)})\\tag{10}$$其中$\\phi$是非线性映射函数，将低维向量映射到高维向量。\n",
    "\n",
    "\n",
    "- 如二维空间的向量$x=(x_1,x_2)^T,z=(z_1,z_2)^T$：\n",
    "$$\\begin{align}\n",
    "K(x,z)&=(x^Tz)^2=(x_1z_1+x_2z_2)^2\\\\\n",
    "&=x_1^2z_1^2+2x_1z_1x_2z_2+x_2^2z_2^2\\\\\n",
    "&=(x_1^2,\\sqrt{2}x_1x_2,x_2^2)(z_1^2,\\sqrt{2}z_1z_2,z_2^2)^T\\\\\n",
    "&=\\phi(x)^T\\phi(z)\n",
    "\\end{align}$$\n",
    "可知当核函数为$K(x,z)=(x^Tz)^2$时，存在非线性映射函数$\\phi(x)=(x_1^2,\\sqrt(2)x_1x_2,x_2^2)$将低维空间向量映射到高维空间。\n",
    "\n",
    "\n",
    "- 推广到核矩阵($K_{ij}=K(x^{(i)},x^{(j)})$)，如何判断核矩阵是否有效(即存在非线性映射函数,使向量的核函数等于高维空间中向量的内积)\n",
    "\n",
    "    - 当核矩阵是半正定矩阵时。  \n",
    "    \n",
    "    - 当核矩阵是由简单有效的核矩阵组成的，设$k_1(x,x'),k_2(x,x')$是有效核矩阵，则以下核矩阵均有效:<img src=\"../img/kernel.png\" \"width=50%\"/>  \n",
    "\n",
    "\n",
    "- 常用有效核函数：\n",
    "    \n",
    "    - $k(x,z)=x^Tz+c,c>0$，利用半正定矩阵的性质及6.20易证\n",
    "    \n",
    "    - $k(x,z)=(x^Tz+c)^n,c>0$，利用上式及6.18可证\n",
    "    \n",
    "- sklearn包中的核函数及核参数：\n",
    "    \n",
    "    - $k(x,z)=x^Tz, linear~kernel$\n",
    "    \n",
    "    - $k(x,z)=(\\gamma x^Tz+r)^d, \\gamma>0, polinomial~kernel$\n",
    "    \n",
    "    - $k(x,z)=tanh(\\gamma x^Tz+r),sigmoid$\n",
    "    \n",
    "    - $k(x,z)=exp(-\\gamma||x-z||^2),\\gamma>0, radial~basis~function(rbf)$，可映射到无穷维空间。代码形式实现的rbf和矩阵形式实现的形式分别如下：\n",
    "    \n",
    "        - 代码形式: 设$X^T=[x^1,x^2]，Z^T=[z^1,z^2,z^3]$\n",
    "        $$\\left[\\begin{matrix}\n",
    "        ||x^1-z^1|| & ||x^1-z^2|| & ||x^1-z^3||\\\\\n",
    "        ||x^2-z^1|| & ||x^2-z^2|| & ||x^2-z^3||\n",
    "        \\end{matrix}\\right]=norm\\left(\\left[\\begin{matrix}\n",
    "        x^1 & x^1 &x^1\\\\\n",
    "        x^2 & x^2 &x^2\n",
    "        \\end{matrix}\\right]-\\left[\\begin{matrix}\n",
    "        z^1 & z^1 \\\\\n",
    "        z^2 & z^2 \\\\\n",
    "        z^3 & z^3 \n",
    "        \\end{matrix}\\right]^T\\right)$$\n",
    "        \n",
    "        - 矩阵形式：\n",
    "        $$X^T=\\left[x^1,x^2\\right]=\\left[\\begin{matrix}\n",
    "        x^1_1 & x^2_1\\\\\n",
    "        x^1_2 & x^2_2\\\\\n",
    "        x^1_3 & x^2_3\n",
    "        \\end{matrix}\\right]$$\n",
    "        $$Z^T=\\left[z^1,z^2,z^3\\right]=\\left[\\begin{matrix}\n",
    "        z^1_1 & z^2_1 & z^3_1\\\\\n",
    "        z^1_2 & z^2_2 & z^3_2\\\\\n",
    "        z^1_3 & z^2_3 & z^3_3\n",
    "        \\end{matrix}\\right]$$求$X,Z$矩阵中的向量两两之间的欧式距离，先求$XZ^T$:\n",
    "        $$XZ^T=\\left[\\begin{matrix}x^1\\\\x^2\\end{matrix}\\right]\\left[z^1,z^2,z^3\\right]=\\left[\\begin{matrix}\n",
    "        x^1z^1 & x^1z^2 & x^1z^3\\\\\n",
    "        x^2z^1 & x^2z^2 & x^2z^3\n",
    "        \\end{matrix}\\right]\n",
    "        $$然后对$X,Z$中的向量求模并扩展：\n",
    "        $$X_{sq}=\n",
    "        $$\n",
    "    \n",
    "   其中$\\gamma, r,d$是核参数，默认值：$\\gamma=1/n\\_features,r=1,d=3,n\\_features是特征维度$。\n",
    "    \n",
    " \n",
    "- 当一种线性方法可以通过输入样本向量间的点积表示时，可以通过核方法将该线性方法扩展到非线性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A B^{T}=\\left[ \\begin{array}{ccc}{\\sum_{k=1}^{3} a_{1}^{k} b_{1}^{k}} & {\\sum_{k=1}^{3} a_{1}^{k} b_{2}^{k}} & {\\sum_{k=1}^{3} a_{1}^{k} b_{3}^{k}} \\\\ {\\sum_{k=1}^{3} a_{2}^{k} b_{1}^{k}} & {\\sum_{k=1}^{3} a_{2}^{k} b_{2}^{k}} & {\\sum_{k=1}^{3} a_{2}^{k} b_{3}^{k}}\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征值分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 方阵的特征值分解：\n",
    "\n",
    "    - $Ax=\\lambda x$\n",
    "    \n",
    "    - 矩阵形式(二维为例)：$$AX=A(x_1,x_2)=(Ax_1,Ax_2)=(\\lambda_1x_1,\\lambda_2x_2)=(x1,x2)\\left[\\begin{matrix}\\lambda_1 & 0\\\\0 &\\lambda_2\\end{matrix}\\right]=X\\Sigma$$\n",
    "\n",
    "\n",
    "- 奇异值分解（Singular Value Decomposition)是特征分解在任意矩阵上的推广:\n",
    "\n",
    "    - $U^TU=I_{m\\times m},V^TV=I_{n\\times n}$，$\\Sigma$是由大到小排列的对角矩阵,对角是奇异值(如果m<n,则有n个奇异值，m个非零奇异值)，$\\Sigma$与$M$维度相同:$$M_{m\\times n}=U_{m\\times m}\\Sigma_{m\\times n} V_{n\\times n}^T\\tag{11}$$\n",
    "    \n",
    "    - $U$的列是$MM^T$的特征向量，$\\Sigma\\Sigma^T$是特征值矩阵: $$\\begin{align}MM^TU&=U\\Sigma V^TV\\Sigma^TU^TU\\\\&=U\\Sigma\\Sigma^T\\end{align}$$\n",
    "    \n",
    "    - $V$的列是$M^TM$的特征向量，$\\Sigma^T\\Sigma$是特征值矩阵：$$\\begin{align}M^TMV&=V\\Sigma^TU^TU\\Sigma V^TV\\\\&=V\\Sigma^T\\Sigma\\end{align}$$\n",
    "    \n",
    "   \n",
    "- 从线性变换的角度看特征分解：\n",
    "\n",
    "    - 线性空间：对加法和数乘封闭的集合，包括多项式空间，向量空间，矩阵空间。以互相正交特征向量作为基底的线性空间是向量空间（空间维度是互相正交的特征向量数）。\n",
    "    \n",
    "    - 线性变换是指在线性空间中对元素（向量）做加法或数乘。矩阵乘以向量表示对该向量的线性变换。即线性变换保持向量空间的线性关系。例如，线性变换总是把直线变成直线，把三角形变成三角形，把平行四边形变成平行四边形。\n",
    "    \n",
    "    - 特征分解相当于在特征（向量）空间中对特征向量进行拉伸（乘法因子是对应的$\\lambda$）\n",
    "    \n",
    " \n",
    "- 从坐标系的角度看特征分解：\n",
    "\n",
    "    - $Ax=y$其中矩阵A的列向量看作向量空间的基底，向量x是在矩阵A描述下的坐标系下的向量，向量y是在单位矩阵描述下的坐标系下的向量。证明：设n维空间有两组基底$A=(a_1,a_2...a_n),I=(i_1,i_2...i_n)$，向量x在坐标系$A$下的表示为$x_a$，在单位坐标系$I$下的表示为$x_i$，则有:$$\\begin{align}x & =m_1i_1+m_2i_2+...m_ni_n\\\\ & =n_1a_1+n_2a_2+...n_na_n\\end{align}$$$$\\Rightarrow\\begin{align}x & = (a_1,a_2...a_n)x_a\\\\ & = (i_1,i_2...i_n)x_i\\end{align}$$其中：$$x_i=(m_1,m_2...m_n)^T\\\\x_a=(n_1,n_2...n_n)^T$$$$\\Rightarrow Ax_a=Ix_i$$\n",
    "    \n",
    "    - 向量与基底的内积表示向量在该基底的投影长度，证明向量$x$在$a_1$的投影为$n_1$:$$\\begin{align}xa_1^T &=(n_1a_1+n_2a_2+...n_na_n)a_1^T\\\\ &=n_1\\end{align}$$\n",
    "    \n",
    "    - $Ax=\\lambda x$表示特征向量x是在坐标系$A$下的向量表示，$\\lambda x$表示同一向量在$I$表示的坐标系下的向量表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-01T12:43:19.691Z"
    }
   },
   "source": [
    "__PCA降维__：通过线性变换，将特征投影到各不相关的维度（协方差为0），选取信息量较大（方差大）的维度作为新的特征；也即通过空间变化，找到更低维的空间来表示样本的特征，并保证低维的特征表示有足够的信息量。变换后的样本集具有以下特点：\n",
    "\n",
    "- 样本各维度特征的方差足够大，去除没有特征或特征不具代表性的那一维（方差小）\n",
    "\n",
    "\n",
    "- 特征间相关性小，去除不同维度之间的冗余信息，如车辆行驶分别以km/h和m/s计算的速度特征（协方差大），降低其中一维。故PCA适合特征间具有线性冗余的降维。\n",
    "\n",
    "\n",
    "- 将具体的特征表示转换为抽象的表示，如变换前的特征是像速度，加速度一样可以具体表示的特征，变换后特征将变得不可描述，但是不同特征的冗余信息更少，且特征更加具有代表性，可用于后续的分类。\n",
    "\n",
    "\n",
    "__PCA推导__：\n",
    "\n",
    "- 给定样本集：$\\{x^{(i)};i=1,2...n\\}$，其中$~x^{(i)}\\in~R^m$,$~x^{(i)}_1,~x^{(i)}_2...x^{(i)}_m~$分别表示样本$~x^{(i)}~$的不同维度特征。\n",
    "\n",
    "\n",
    "- 均值归一化和方差归一化，使各维度可比较（以下计算均为element-wise,单个样本列向量表示）:$$\\mu=\\frac{1}{n}\\sum\\limits_{i=1}^nx^{(i)}\\\\\\sigma^2=\\frac{1}{m}\\sum\\limits_{i=1}^n(x^{(i)}-\\mu)^2\\\\x~^{(i)}\\leftarrow\\frac{x^{(i)}-\\mu}{\\sigma}$$\n",
    "\n",
    "\n",
    "- 令投影的方向向量为$~u~$,其中$||u||_2=1$,且满足样本在该方向有最大方差，则最大化：$$\\begin{align}\\frac{1}{n}\\sum\\limits_{i=1}^n(x^{(i)^T}u)^2 &=\\frac{1}{n}\\sum\\limits_{i=1}^nu^Tx^{(i)}x^{(i)^T}u\\\\&=u^T\\big(\\frac{1}{n}\\sum\\limits_{i=1}^nx^{(i)}x^{(i)^T}\\big)u\\\\&=u^T\\Sigma~u\\end{align}$$\n",
    "\n",
    "\n",
    "- 根据(5)易知$~\\Sigma~$是该样本集关于各维特征的协方差矩阵。根据拉格朗日数乘，有：$$L=\\frac{1}{n}\\sum\\limits_{i=1}^nu^Tx^{(i)}x^{(i)^T}u-\\lambda(u^Tu-1)$$令$\\frac{\\partial L}{\\partial u}=0$，根据(6)和(8)有：$$\\Sigma u=\\lambda u$$\n",
    "\n",
    "\n",
    "- 易知当$u$和$\\lambda$分别为$\\Sigma$的特征向量和特征值时，样本在$u$上的投影的方差有最大值。\n",
    "\n",
    "\n",
    "__步骤__:\n",
    "\n",
    "- 给定样本集：$X^T=[x^{(1)},x^{(2)}...x^{(n)}]$\n",
    "\n",
    "\n",
    "- 样本零均值化(为方便后续分类，最好同时方差归一化):$$x^{(i)}\\leftarrow x^{(i)}-\\frac{1}{n}\\sum_{i=1}^nx^{(i)}\\\\矩阵形式：[x^{(1)},x^{(2)}...x^{(n)}]\\leftarrow [x^{(1)},x^{(2)}...x^{(n)}]-\\frac{1}{n}[x^{(1)},x^{(2)}...x^{(n)}]\\left[\\begin{matrix}1\\\\1\\\\...\\\\1\\end{matrix}\\right][1,1...1]\\\\\\Rightarrow X^T\\leftarrow X^T-X^T1_N$$\n",
    "\n",
    "\n",
    "- 求出样本集协方差矩阵：$\\Sigma=\\frac{1}{n-1}\\sum\\limits_{i=1}^n(x^{(i)}x^{(i)^T})=\\frac{1}{n-1}[x^{(1)},x^{(2)}...x^{(n)}]\\left[\\begin{matrix}(x^{(1)})^T\\\\(x^{(2)})^T\\\\...\\\\(x^{(n)})^T\\end{matrix}\\right]=\\frac{1}{n-1}X^TX$\n",
    "\n",
    "\n",
    "- 求出协方差矩阵$\\Sigma$的所有特征值$\\lambda$和对应特征向量$u$，根据累计贡献率$k=\\frac{\\sum_{i=1}^k\\lambda_i}{\\sum_{i=1}^n\\lambda_i}$选出前k个主成分组成$U_k^T=(u_1,u_2...u_k)$\n",
    "\n",
    "\n",
    "- 将样本集投影至各主成分：$$(x^{(i)}_{rot})^T=(x^{(i)})^T[u_1,u_2...u_k]=(x^{(i)})^TU_k^T\\\\矩阵形式：X_{rot}=\\left[\\begin{matrix}(x^{(1)}_{rot})^T\\\\(x^{(2)}_{rot})^T\\\\...\\\\(x^{(n)}_{rot})^T\\end{matrix}\\right]=\\left[\\begin{matrix}(x^{(1)})^T\\\\(x^{(2)})^T\\\\...\\\\(x^{(n)})^T\\end{matrix}\\right]U_k^T=XU_k^T$$\n",
    "\n",
    "\n",
    "__还原__：\n",
    "\n",
    "- $X=XU_k^TU_k=X_{rot}U_k$\n",
    "\n",
    "\n",
    "\n",
    "__通过SVD分解__:\n",
    " 可以不通过协方差矩阵求特征值，直接对原数据进行SVD分解，见(11)，利用SVD进行PCA降维更加高效，协方差的维度更高，对协方差特征分解效率比SVD更慢。\n",
    "\n",
    "- 给定样本集$~X^T=[x^{(1)},x^{(2)}...x^{(n)}]~$，$X$适合编程的矩阵表示形式($n\\times m$)，$[x^{(1)},x^{(2)}...x^{(n)}]$适合书面矩阵表示形式。\n",
    "\n",
    "\n",
    "- 中心化：$$x^{(i)}\\leftarrow x^{(i)}-\\frac{1}{n}\\sum_{i=1}^nx^{(i)}\\\\矩阵形式：[x^{(1)},x^{(2)}...x^{(n)}]\\leftarrow[x^{(1)},x^{(2)}...x^{(n)}]-\\frac{1}{n}[x^{(1)},x^{(2)}...x^{(n)}]\\left[\\begin{matrix}1\\\\1\\\\...\\\\1\\end{matrix}\\right][1,1...1]\\\\\n",
    "\\begin{align}&\\Rightarrow X^T\\leftarrow X^T-X^T1_N\\\\&\\Rightarrow X\\leftarrow X-1_NX，其中1_N\\in R^{n\\times n},(1_N)_{ij}=\\frac{1}{n}\\end{align}$$\n",
    "\n",
    "\n",
    "- SVD分解：$X=U\\Sigma V^T$，其中$U,V$的列是向量。根据(11)知$V$是$X^TX$的特征向量矩阵。根据特征值矩阵$\\Sigma^T\\Sigma$(特征值需归一化)选择$k$，并选出前$k$个特征向量$V_k=[v_1,v_2...v_k]$。\n",
    "\n",
    "\n",
    "- 投影：$X_{rot}=XV_k=U\\Sigma V^TV_k=U\\Sigma\\left[\\begin{matrix}E_k\\\\0\\end{matrix}\\right]=U[\\Sigma_k,\\Sigma_r]\\left[\\begin{matrix}E_k\\\\0\\end{matrix}\\right]=U(\\Sigma_kE_k+\\Sigma_r0)=U\\Sigma_k$，其中$\\Sigma_k$是前$k$列向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__KPCA降维__:\n",
    "\n",
    "PCA是线性映射方法，只能消除特征间的线性冗余(新特征是原有特征的线性叠加)，忽略了特征间高于2阶的非线性关系。KPAC将样本映射至一个随机的高维空间，特征间的非线性关系在高维空间中变成了线性，此时再利用PCA对高维样本进行降维，可以消除特征间的非线性冗余。\n",
    "\n",
    "\n",
    "__KPCA推导__:\n",
    "\n",
    "- 给定样本集：$\\{x^{(i)};i=1,2...n\\}$,其中$~x^{(i)}\\in~R^m$，引入非线性映射函数$~\\phi~$,将样本$~x^{(i)}~$映射到高维空间$~F~$,其中$~\\phi~$的具体形式和$~F~$的维度不需要知道：$$\\begin{align}\\phi: &R^m~\\rightarrow~F\\\\&x~\\rightarrow~X\\end{align}$$为方便描述,其中小写符号表示$R^m$空间的向量，大写表示$F$空间的向量。假设映射后的样本已中心化，即$\\sum_{i=1}^n\\phi(x^{(i)})=0$。\n",
    "\n",
    "\n",
    "- 在$F$空间进行PCA降维，协方差矩阵可表示为：\n",
    "$$\\overline{C}=\\frac{1}{n}\\sum\\limits_{i=1}^n\\phi({x^{(i)})(\\phi(x^{(i)})}^T$$\n",
    "\n",
    "\n",
    "- 协方差矩阵的特征值$\\lambda$和特征向量$V$：\n",
    "$$\\overline{C}V=\\lambda V$$\n",
    "\n",
    "\n",
    "- $V$可由$\\phi(x^{(i)}),i=1,2...n$线性表示：\n",
    "$$\\begin{align}\n",
    "V &=\\frac{1}{\\lambda }\\overline{C}V \\\\\n",
    "  &=\\frac{1}{\\lambda n}\\sum\\limits_{i=1}^n\\phi(x^{(i)})\\phi(x^{(i)})^TV\\\\\n",
    "  &=\\sum\\limits_{i=1}^na_i\\phi(x^{(i)})\n",
    "\\end{align}$$\n",
    "其中$a_i=\\frac{1}{\\lambda n}\\phi(x^{(i)})^TV$，故$V\\in span\\{\\phi(x^{(1)}),\\phi(x^{(2)})...\\phi(x^{(n)})\\}$。\n",
    "\n",
    "\n",
    "- $\\overline{C}V=\\lambda V$可重新表示为：\n",
    "$$\\lambda\\sum_{i=1}^na_i\\phi(x^{(i)})=\\frac{1}{n}\\sum\\limits_{i=1}^n\\phi(x^{(i)})\\phi(x^{(i)})^T\\sum_{j=1}^na_j\\phi(x^{(j)})$$\n",
    "$\\forall k\\in[1,n]$,两边同时左乘$\\phi(x^{(k)})^T$:\n",
    "$$\\lambda\\sum_{i=1}^na_i\\phi(x^{(k)})^T\\phi(x^{(i)})=\\frac{1}{n}\\phi(x^{(k)})^T\\sum\\limits_{i=1}^n\\phi(x^{(i)})\\phi(x^{(i)})^T\\sum_{j=1}^na_j\\phi(x^{(j)})$$\n",
    "根据(10),令$K_{ij}=\\phi(x^{(i)})^T\\phi({x^{(j)}})$，上式左边：\n",
    "$$\\lambda\\sum_{i=1}^na_i\\phi(x^{(k)})^T\\phi(x^{(i)})=\\lambda\\sum\\limits_{i=1}^na_iK_{ki}=\\lambda(Ka)_k$$\n",
    "其中$(Ka)_k$表示矢量的第$k$个元素。根据(9)，上式右边：\n",
    "$$\\begin{align}\n",
    "&\\frac{1}{n}\\phi(x^{(k)})^T\\sum\\limits_{i=1}^n\\phi(x^{(i)})\\phi(x^{(i)})^T\\sum_{j=1}^na_j\\phi(x^{(j)})\\\\\n",
    "&=\\frac{1}{n}\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^na_j\\phi(x^{(k)})^T\\phi(x^{(i)})\\phi({x^{(i)}})^T\\phi(x^{(j)})\\\\\n",
    "&=\\frac{1}{n}\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^na_jK_{ki}K_{ij}=\\frac{1}{n}\\sum\\limits_{i=1}^n(Ka)_iK_{ki}\\\\\n",
    "&=\\frac{1}{n}(K^2a)_k\n",
    "\\end{align}$$\n",
    "$\\because\\forall k\\in[1,n]$，左边=右边，$\\therefore~\\lambda Ka=\\frac{1}{n}K^2a~\\rightarrow~n\\lambda a=Ka$，可知$\\lambda_k=n\\lambda$和$a$分别为$K$的特征值和特征向量。\n",
    "\n",
    "\n",
    "- 根据$V_k^TV_k=1$，对$a$归一化：\n",
    "$$\\begin{align}\n",
    "&1=\\sum\\limits_{i=1}^na_i\\phi(x^{(i)})^T\\sum\\limits_{j=1}^na_j\\phi(x^{(j)})\\\\\n",
    "&=\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^na_ia_j\\phi(x^{(i)})^T\\phi(x^{(j)})\\\\\n",
    "&=\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^na_ia_jK_{ij}=\\sum\\limits_{i=1}^na_i(Ka)_i\\\\\n",
    "&=a^TKa=\\lambda_ka^Ta\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "- 计算单个样本在一个核主元上的投影：\n",
    "$$\\begin{align}\n",
    "&\\phi(x^{(i)})^TV=\\sum\\limits_{j=1}^na_j\\phi(x^{(i)})^T\\phi(x^{(j)})\\\\\n",
    "&=\\sum\\limits_{j=1}^na_jK_{ij}=(Ka)_i\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "__KPCA步骤__：\n",
    "\n",
    "- 给定样本集：$X^T=[x^{(i)},x^{(2)}...x^{(n)}]$\n",
    "\n",
    "\n",
    "- 选择核函数$k$\n",
    "\n",
    "\n",
    "- 计算核矩阵$K$(维度基于样本维度，而PCA基于特征维度):\n",
    "$$K_{ij}=k(x^{(i)},x^{(j)})$$\n",
    "\n",
    "\n",
    "- 均值归一化$K$(矩阵运行规则见Sec 3)：\n",
    "$$\\begin{align}\n",
    "&\\overline{K}_{ij}=(\\phi(x^{(i)})-\\frac{1}{n}\\sum\\limits_{l=1}^n\\phi(x^{(l)}))^T(\\phi(x^{(j)})-\\frac{1}{n}\\sum\\limits_{k=1}^n\\phi(x^{(k)}))\\\\\n",
    "&=\\phi(x^{(i)})^T\\phi(x^{(j)})-\\frac{1}{n}\\sum\\limits_{l=1}^n\\phi(x^{(l)})^T\\phi(x^{(j)})-\\frac{1}{n}\\sum\\limits_{k=1}^n\\phi(x^{(i)})^T\\phi(x^{(k)})+\\frac{1}{n^2}\\sum\\limits_{l=1}^n\\sum\\limits_{k=1}^n\\phi(x^{(l)})^T\\phi(x^{(k)})\\\\\n",
    "&=K_{ij}-\\frac{1}{n}\\sum\\limits_{l=1}^nK_{lj}-\\frac{1}{n}\\sum\\limits_{k=1}^nK_{ik}+\\frac{1}{n^2}\\sum\\limits_{l=1}^n\\sum\\limits_{k=1}^nK_{lk}\\\\\n",
    "&\\Rightarrow\\overline{K}=K-1_NK-K1_N+1_NK1_N\n",
    "\\end{align}$$\n",
    "其中$1_N\\in R^{N\\times N},(1_N)_{ij}=1/n$。\n",
    "\n",
    "\n",
    "- 计算$\\overline{K}$的特征值$\\lambda_k$及特征向量$a$\n",
    "\n",
    "\n",
    "- $a$规则化:\n",
    "$$a^Ta=\\frac{1}{\\lambda_k}\\\\\n",
    "\\Rightarrow a\\leftarrow\\frac{1}{\\sqrt{\\lambda_k}}a\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- 将样本集投影至各主成分，新的样本集可表示为$\\{\\overline{K}a^{(1)},\\overline{K}a^{(2)}...\\overline{K}a^{(k)}\\}$，其中$k$是选择的主元数。\n",
    "\n",
    "\n",
    "__新样本点在原数据集特征空间的投影__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 白化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "210px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
