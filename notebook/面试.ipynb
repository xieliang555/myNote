{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __项目中遇到的问题？如何解决__？\n",
    "\n",
    "    - KPCA+CNN： 将KPCA应用于神经网络的中间层有困难\n",
    "        - 1）PCA降维维度太大（3200），特征分解很慢有时求不出，改用KPCA，通过选择样本的数量控制维度。\n",
    "        - 2）将KPCA应用与神经网络导致神经元个数无法确定。\n",
    "        - 3）新的样本如何投影到训练样本的特征空间中。看sklearn源码\n",
    "        - 3）实现KPCA时要调用SVD分解的包，tensorflow不支持对SVD的反向传播。\n",
    "        \n",
    "    - DQN+Q-learning：奖励函数\n",
    "        - 1）设置奖励函数，经常不收敛，如Q-learning：根据一些非线性函数设计奖励函数，往往不收敛或者没有想要的结果。设计稀疏的奖励函数，但学起来更慢。DQN的奖励不能太大，神经网络的输出很小，奖励太大会掩盖住神经网络的输出。\n",
    "        - 2）奖励函数的调试，检查奖励的有效性：状态动作空间太大，奖励的反馈存在延迟，很难观察学习到的策略是如何学习的。观察终止状态的学习过程，检查奖励函数的有效性和代码的逻辑性。\n",
    "        \n",
    "    - KPCA+MLP：\n",
    "        - 测试的样本如何降维？\n",
    "        - 第一个项目，无法通过结果（降维效果）检查代码是否有逻辑上的错误。自己实现的算法和python包调用的算法在相同的样本上实现，看是否相同\n",
    "\n",
    "\n",
    "- __如何调参数？哪些调参数算法__？（网格搜索，随机搜索，贝叶斯优化）\n",
    "    - 手动调参数：控制变量，先粗调后微调，调好一个参数后再调其他的，参数一般先从默认值开始，如lr：1e-4，filter个数一般选2的倍数：32，64，优化算法选SGD/Adam，dropout标配\n",
    "    - 基于机器辅助：Talos，通过比较不同超参数与预测精度的相关性，逐渐减小超参数的可选范围。\n",
    "    - 基于算法：Spearmint（高斯过程代理）,SMAC（随机森林回归）,Hyperopt（随机，gridsearch，tpe），Scikit-Optimize(贝叶斯优化)\n",
    "    \n",
    "    \n",
    "- __如何评估模型泛化能力（过拟合/欠拟合，偏差/方差）？__\n",
    "    - 模型容量（复杂度）低于最优容量就是欠拟合，高于最优容量就是过拟合\n",
    "    - 当模型复杂度比较小时，模型的学习能力很差，预测的整体偏差（误差期望）会比较大，模型对个别采样点的扰动不会很敏感（模型对个别样本的个性的学习能力比较差），所以方差会比较小；随着模型复杂度变大，学习能力增强，可以学习到样本集的全局性（共性），偏差会慢慢减小，方差也不会很大，此时模型容量最优；当模型复杂度进一步增大，偏差更小，对样本个性也更敏感，甚至偏差为0，即完全拟合了样本集，此时方差最大，将数据的波动（噪声）也学习了。\n",
    "\n",
    "- __有哪些深度神经网络?特点与解决的问题__ \n",
    "    - LeNet\n",
    "    - AlexNet\n",
    "    - VGGNet\n",
    "    - GoogleNet\n",
    "    \n",
    "- __CNN如何减少参数?__\n",
    "    - 通过卷积核将全连接替换为局部连接\n",
    "    - 权值共享：同一个特征图共享一个卷积核\n",
    "    - 池化减少了展开后神经元的个数\n",
    "    \n",
    "- __为什么参数少了反而提高学习能力？__\n",
    "    - 由于权值共享，卷积核可以提取全局的局部特征（与局部特征的位置无关），较好的保留了局部特征的平面结构信息\n",
    "    - 参数减少可以避免模型过拟合，提高泛化能力。\n",
    "\n",
    "- __如何减少过拟合__？\n",
    "    - L1/L2正则\n",
    "    - dropout\n",
    "    - early stoping\n",
    "    - 减小模型复杂度\n",
    "    - 减小训练epoch\n",
    "    - 丰富样本\n",
    "\n",
    "- __数据预处理__：\n",
    "    - 均值归一化\n",
    "    - 方差归一化\n",
    "    - PCA/KPCA降维\n",
    "    - 白化\n",
    "\n",
    "- __正确率（precision）和召回率（recall）？__ \n",
    "    - 假设有20个1，10个0，一次测试中预测5个为1，这个5个预测值中有3个的真实类别是1，2个的真实类别是0，则正确率为3/5，召回率为3/20；25个为0，这25个预测值中有17个真实类别是1，8个是0，则正确率为8/25，召回率为8/10。\n",
    "    \n",
    "\n",
    "- __项目介绍__\n",
    "    - CNN+KPCA: \n",
    "        - 车辆工况识别，给定车辆在不同工况下的样本，利用CNN+KPCA对不同的工况进行识别。\n",
    "    - DQN+Q-learning:\n",
    "        - 车辆混合能源管理，电动汽车一般是锂电池作为主能源，然后超级电容作为辅助能源弥补锂电池在车辆加速阶段的动力不足，同时在制动阶段又能吸收制动能量，目标是设计一个控制算法对锂电池和超级电容之间的供能进行合理分配，能满足驾驶员的功率需求，同时又使耗能最低，另外还要考虑降低锂电池的输出频率以延长电池的使用寿命。为满足上述目标，考虑使用DQN和Q-learning分别控制策略进行学习。\n",
    "    - KPCA+MLP：\n",
    "        - 车辆工况识别，给定车辆在某一典型工况下的行驶速度，以这一时间段的速度信息代表该工况，进行特征提取（平均速度，加速度，停车时间等），对特征进行KPCA降维，然后利用神经网络对降维后的特征进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Python常用库__:\n",
    "    - Pickle：结构化数据的存取；Pandas：数据预处理；NumPy：数值计算；scikit-learn：机器学习库；Matplotlab：可视化；Pillow：图形库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rbf和mlp的区别： chrome-note(rbf和mlp的区别，nn，rbf神经网络)\n",
    "- 神经网络输入归一化/标准化：chrome-note(nn)\n",
    "- 数据预处理：normalizing([0,1],[-1,1]),standardizing(随机变量服从正太分布，x->(x-u)/std)，测试集标准化的数据必须来自训练集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "## Forward propagation\n",
    "- 什么是独立同分布？  \n",
    "1.独立：前后样本的出现不会互相影响（不要用连续的样本取训练，随机梯度下降可以解决）。  \n",
    "2.同分布：前后样本的出现是随机的，满足同一分布律或分布函数（一般能满足）。  \n",
    "  \n",
    "  \n",
    "- 训练神经网络的样本为什么要独立同分布？    \n",
    "1.连续相关的样本用于训练会导致神经网络对最近训练的样本过拟合，而忘了之前学习到的特征。由于拟合了最近的一些噪声使参数更新的方差（variance）比较大。独立的数据打破样本之间的相关性，通过随机采样互相抵消之间的噪声，加快收敛。  \n",
    "2.用连续相关的样本训练神经网络，算法在连续一段时间内基本朝着同一个方向做gradient descent（拟合噪声），甚至会导致不收敛。 \n",
    "  \n",
    "  \n",
    "- 神经元的本质（不同神经元代表不同的维度）？  \n",
    "  \n",
    "  \n",
    "- 神经网络w和b的作用（简化至二维平面w和b的作用） （本质上是坐标的线性变换，寻找不同的空间以实现分类）？   \n",
    "  \n",
    "  \n",
    "- 激活函数为什么需要sigmoid/relu等非线性激活函数（本质上是非线性变换）？  \n",
    "<img src=\"img/1.png\" width=\"50%\">\n",
    "  \n",
    "  \n",
    "- sigmoid和relu的区别？神经元的输出为什么总是正的？    \n",
    "  \n",
    "  \n",
    "- 如何初始化参数？  \n",
    "1.阈值添加少量噪声打破隐藏对称性。  \n",
    "2.对于relu用一个较小正数初始化偏置项  \n",
    "     \n",
    "  \n",
    "  \n",
    "- 旋转不变性和平移不变性？  \n",
    "  \n",
    "  \n",
    "- dropout如何操作及其作用？   \n",
    "1.[dropout.pdf](http://localhost:8888/files/pdf/dropout.pdf)  \n",
    "2.训练时需要dropout，测试时不需要  \n",
    "3.输出层是线性输出时权值需要乘以drop的比例，输出层是softmax时权值不需要改变。  \n",
    "\n",
    "\n",
    "\n",
    "## Loss function\n",
    "- cross-entropy(最大似然) [cross_entropy.pdf](http://localhost:8888/files/pdf/cross_entropy.pdf) [loss_function.pdf](http://localhost:8888/files/pdf/loss_function.pdf) [visual_information.pdf](http://localhost:8888/files/pdf/visual_information.pdf)  \n",
    "  \n",
    "  \n",
    "- 交叉墒为什么要取对数？     \n",
    "1.乘法变加法，简化计算  \n",
    "2.要求导的话很多常数项可以省去，简化分析  \n",
    "3.大量很小的因子（如概率）直接相乘结果接近0，会产生下溢，计算机直接近似为0，取对数可避免    \n",
    "\n",
    "## Back propagation\n",
    "- 训练为什么需要batch？  \n",
    "1.减少计算开销。  \n",
    "2.最大化地学习到数据集的总体特性。  \n",
    "  \n",
    "  \n",
    "- 优化方法有哪些，如何选择？\n",
    "\n",
    "## 调参经验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL\n",
    "- on-policy和off-policy的区别？      \n",
    "1.在更新Q-table时，on-policy采用greedy选择下一状态的动作，off-policy采用max选择下一状态的动作  \n",
    "2.on-policy通过greedy选择下一状态的动作；off-policy通过greedy选择当前状态的动作。     \n",
    "两者在效果上的区别(实验测试)...  \n",
    "  \n",
    "  \n",
    "- 经验回放为什么只能用于off-policy？ \n",
    "\n",
    "\n",
    "- 为什么要分$\\theta$和$\\theta'$?\n",
    "  \n",
    "  \n",
    "- 经验回放（ER）的作用？    \n",
    "1.获取训练样本；  \n",
    "2.样本可以重复利用进行参数训练；  \n",
    "3.打破数据相关性，减少参数更新的方差，减少不稳定，防止训练不收敛；      \n",
    "4.防止对最近的经验过拟合，过快的忘记一些很少出现但有用的经验（PER）  \n",
    "  \n",
    "  \n",
    "- 重要性采样（IS）  \n",
    "  \n",
    "  \n",
    "- $\\gamma$的意义？  \n",
    "$Q(s,a)=\\mathbb{E}[r_{t+1}+\\gamma r_{t+2}+\\gamma^2r_{t+3}+...|s=s_t,a=a_t]$   \n",
    "在更新Q值时，用来衡量当前reward和将来的reward的重要性，$\\gamma$大,考虑将来的奖励的比重加大  \n",
    "\n",
    "\n",
    "- tabular相比nonlinear approximation function的区别？  \n",
    "当状态空间和动作空间维度较大，tabular的方法学起来相对困难，很多Q值可能学习不到。 AF只需要学习权值  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.6]",
   "language": "python",
   "name": "conda-env-py3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
