{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:35:49.483332Z",
     "start_time": "2019-05-24T08:35:44.430493Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import accumulate\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "\n",
    "class VPGBuffer:\n",
    "    def __init__(self, obs_dim, act_dim=1, size,gamma=0.99,lam=0.95):\n",
    "        # obs_dim和act_dim应该可以兼容不同的环境\n",
    "        self.obs_buf=np.zeros(shape=(size,obs_dim),dtype=np.float32)\n",
    "        # 为什么要保存act，而不直接保存logpi的梯度？（效率太低，无法进行批量梯度下降）\n",
    "        self.act_buf=np.zeros(shape=(size,),dtype=np.float32)\n",
    "        self.rew_buf=np.zeros(shape=(size,),dtype=np.float32)\n",
    "        self.val_buf=np.zeros(shape=(size,),dtype=np.float32)\n",
    "        self.logp_buf=np.zeros(shape=(size,),dtype=np.float32)\n",
    "        self.adv_buf=np.zeros(shape=(size,),dtype=np.float32)\n",
    "        self.ret_buf=np.zeros(shape=(size,),dtype=np.float32)\n",
    "        self.ptr,self.path_start_idx=0,0\n",
    "        self.gamma,self.lam,self.size=gamma,lam,size\n",
    "            \n",
    "            \n",
    "    def store(self,obs,act,rew,val,logp=0):\n",
    "        assert self.ptr<self.size, \\\n",
    "            \"buffer overflowed: size= \"+str(self.ptr)+\" max_size= \"+str(self.size)\n",
    "        self.obs_buf[self.ptr]=obs\n",
    "        self.act_buf[self.ptr]=act\n",
    "        self.rew_buf[self.ptr]=rew\n",
    "        self.logp_buf[self.ptr]=logp\n",
    "        self.val_buf[self.ptr]=val\n",
    "        self.ptr+=1\n",
    "        \n",
    "        \n",
    "    def finish_path(self,last_val=0):\n",
    "        path_slice=slice(self.path_start_idx,self.ptr)\n",
    "        rews=np.append(self.rew_buf[path_slice],last_val)\n",
    "        vals=np.append(self.val_buf[path_slice],last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas=rews[:-1]+self.gamma*vals[1:]-vals[:-1]\n",
    "        self.adv_buf[path_slice]=list(accumulate(deltas[::-1],lambda x,y:x*self.gamma*self.lam+y))[::-1]\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice]=list(accumulate(rews[::-1],lambda x,y:x*self.gamma+y))[::-1][:-1]\n",
    "        self.path_start_idx=self.ptr\n",
    "        \n",
    "        \n",
    "    def get(self):\n",
    "        assert self.ptr==self.size,\\\n",
    "            \"buffer is not full\"\n",
    "        self.ptr,self.path_start_idx=0,0\n",
    "        \n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean,adv_std=np.mean(self.adv_buf),np.std(self.adv_buf)\n",
    "        self.adv_buf=(self.adv_buf-adv_mean)/adv_std\n",
    "        return [self.obs_buf,self.act_buf,self.adv_buf,self.ret_buf]\n",
    "    \n",
    "    \n",
    "\n",
    "def mlp(x,hidden_sizes=(64,64),activation=tf.tanh,output_activation=None):\n",
    "    for i in hidden_sizes[:-1]:\n",
    "        x=tf.layers.dense(x,units=i,activation=activation)\n",
    "    return tf.layers.dense(x,units=hidden_sizes[-1],activation=output_activation)\n",
    "\n",
    "\n",
    "def vpg(env_name,epoch=10,local_steps_per_epoch=4000,seed=0,\n",
    "        pi_lr=3e-4,vf_lr=1e-3,gamma=0.99,lam=0.97,\n",
    "        max_ep_len=1000,train_v_iters=80):\n",
    "    \n",
    "    env=gym.make(env_name)\n",
    "    obs_dim=env.observation_space.shape[0]\n",
    "    act_dim=env.action_space.n\n",
    "    buf=VPGBuffer(obs_dim,act_dim,local_steps_per_epoch,gamma,lam)\n",
    "    \n",
    "    # define graph\n",
    "    tf.set_random_seed(seed)\n",
    "    #输入状态x_ph应能兼容不同环境\n",
    "    x_ph=tf.placeholder(shape=(None,obs_dim),dtype=tf.float32)\n",
    "    #从buffer中获取动作，求logp(a|s),用于训练，(应能兼容连续动作)\n",
    "    a_ph=tf.placeholder(shape=(None,),dtype=tf.int32)\n",
    "    adv_ph=tf.placeholder(shape=(None,),dtype=tf.float32)\n",
    "    ret_ph=tf.placeholder(shape=(None,),dtype=tf.float32)\n",
    "    all_phs=[x_ph,a_ph,adv_ph,ret_ph]\n",
    "    \n",
    "    # define value-function net(s->v)\n",
    "    v=tf.squeeze(mlp(x_ph,hidden_sizes=(64,64,1)),axis=1)\n",
    "    \n",
    "    # define stochastic policy net(s->a)\n",
    "    logits=mlp(x_ph,hidden_sizes=(64,64,act_dim))\n",
    "    logp_all=tf.nn.log_softmax(logits)\n",
    "    # sampling\n",
    "    pi=tf.squeeze(tf.random.categorical(logp_all,1),axis=1)\n",
    "    # log-likelihood\n",
    "    logp=tf.reduce_sum(tf.one_hot(a_ph,depth=act_dim)*logp_all,axis=1)\n",
    "    \n",
    "    # VPG objectives\n",
    "    pi_loss=-tf.reduce_mean(logp*adv_ph)\n",
    "    v_loss=tf.reduce_mean((ret_ph-v)**2)\n",
    "    \n",
    "    # optimizers\n",
    "    train_pi=tf.train.AdamOptimizer(learning_rate=pi_lr).minimize(pi_loss)\n",
    "    train_v=tf.train.AdamOptimizer(learning_rate=vf_lr).minimize(v_loss)\n",
    "    \n",
    "    sess=tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    for epoch in range(1):\n",
    "        o,r,d,ep_len=env.reset(),0,False,0\n",
    "        for t in range(local_steps_per_epoch):\n",
    "            a,v_t=sess.run([pi,v],feed_dict={x_ph:o.reshape(1,-1)})\n",
    "            buf.store(o,a,r,v_t)\n",
    "            o,r,d,_=env.step(a[0])\n",
    "            ep_len+=1\n",
    "            \n",
    "            terminal=d or (ep_len==max_ep_len)\n",
    "            if terminal:\n",
    "                last_val=r if d else sess.run(v,feed_dict={x_ph,o.reshape(1,-1)})\n",
    "                buf.finish_path(last_val)\n",
    "                o,r,d,ep_len=env.reset(),0,False,0\n",
    "                \n",
    "#         buff=buf.get()\n",
    "#         print(buff[0].shape, buff[1].shape, buff[2].shape,buff[3].shape)\n",
    "        inputs={k:v for k,v in zip(all_phs,buf.get())}\n",
    "            \n",
    "        # update per epoch\n",
    "        # policy update\n",
    "        sess.run(train_pi,feed_dict=inputs)\n",
    "        \n",
    "        # value function update\n",
    "        for _ in range(train_v_iters):\n",
    "            sess.run(train_v,feed_dict=inputs)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "                  \n",
    "vpg(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl]",
   "language": "python",
   "name": "conda-env-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
