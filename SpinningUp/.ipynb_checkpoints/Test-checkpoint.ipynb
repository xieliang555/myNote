{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T13:56:16.194426Z",
     "start_time": "2019-06-04T13:53:20.543190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1mLogging data to /tmp/experiments/1559656402/progress.txt\u001b[0m\n",
      "\u001b[36;1mSaving config:\n",
      "\u001b[0m\n",
      "{\n",
      "    \"ac_kwargs\":\t{},\n",
      "    \"act_noise\":\t0.1,\n",
      "    \"actor_critic\":\t\"mlp_actor_critic\",\n",
      "    \"batch_size\":\t100,\n",
      "    \"env_fn\":\t\"<function <lambda> at 0x134966d08>\",\n",
      "    \"epochs\":\t5,\n",
      "    \"gamma\":\t0.99,\n",
      "    \"logger\":\t{\n",
      "        \"<spinup.utils.logx.EpochLogger object at 0x129868080>\":\t{\n",
      "            \"epoch_dict\":\t{},\n",
      "            \"exp_name\":\tnull,\n",
      "            \"first_row\":\ttrue,\n",
      "            \"log_current_row\":\t{},\n",
      "            \"log_headers\":\t[],\n",
      "            \"output_dir\":\t\"/tmp/experiments/1559656402\",\n",
      "            \"output_file\":\t{\n",
      "                \"<_io.TextIOWrapper name='/tmp/experiments/1559656402/progress.txt' mode='w' encoding='UTF-8'>\":\t{\n",
      "                    \"mode\":\t\"w\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"logger_kwargs\":\t{},\n",
      "    \"max_ep_len\":\t1000,\n",
      "    \"noise_clip\":\t0.5,\n",
      "    \"pi_lr\":\t0.001,\n",
      "    \"policy_delay\":\t2,\n",
      "    \"polyak\":\t0.995,\n",
      "    \"q_lr\":\t0.001,\n",
      "    \"replay_size\":\t1000000,\n",
      "    \"save_freq\":\t1,\n",
      "    \"seed\":\t0,\n",
      "    \"start_steps\":\t10000,\n",
      "    \"steps_per_epoch\":\t5000,\n",
      "    \"target_noise\":\t0.2\n",
      "}\n",
      "\n",
      "Number of parameters: \t pi: 129306, \t q1: 130201, \t q2: 130201, \t total: 389708\n",
      "\n",
      "WARNING:tensorflow:From /Users/xieliang/spinningup/spinup/utils/logx.py:226: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From /Users/xieliang/anaconda3/envs/rl/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /tmp/experiments/1559656402/simple_save/saved_model.pb\n",
      "---------------------------------------\n",
      "|             Epoch |               1 |\n",
      "|      AverageEpRet |            -337 |\n",
      "|          StdEpRet |              87 |\n",
      "|          MaxEpRet |            -231 |\n",
      "|          MinEpRet |            -468 |\n",
      "|  AverageTestEpRet |            -559 |\n",
      "|      StdTestEpRet |            2.58 |\n",
      "|      MaxTestEpRet |            -555 |\n",
      "|      MinTestEpRet |            -565 |\n",
      "|             EpLen |           1e+03 |\n",
      "|         TestEpLen |           1e+03 |\n",
      "| TotalEnvInteracts |           5e+03 |\n",
      "|     AverageQ1Vals |            2.19 |\n",
      "|         StdQ1Vals |            2.91 |\n",
      "|         MaxQ1Vals |            16.6 |\n",
      "|         MinQ1Vals |           -7.63 |\n",
      "|     AverageQ2Vals |            2.19 |\n",
      "|         StdQ2Vals |            2.91 |\n",
      "|         MaxQ2Vals |              16 |\n",
      "|         MinQ2Vals |           -7.29 |\n",
      "|            LossPi |           -3.31 |\n",
      "|             LossQ |           0.401 |\n",
      "|              Time |            35.4 |\n",
      "---------------------------------------\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /tmp/experiments/1559656402/simple_save/saved_model.pb\n",
      "---------------------------------------\n",
      "|             Epoch |               2 |\n",
      "|      AverageEpRet |            -281 |\n",
      "|          StdEpRet |              37 |\n",
      "|          MaxEpRet |            -226 |\n",
      "|          MinEpRet |            -328 |\n",
      "|  AverageTestEpRet |            -657 |\n",
      "|      StdTestEpRet |            43.3 |\n",
      "|      MaxTestEpRet |            -584 |\n",
      "|      MinTestEpRet |            -739 |\n",
      "|             EpLen |           1e+03 |\n",
      "|         TestEpLen |           1e+03 |\n",
      "| TotalEnvInteracts |           1e+04 |\n",
      "|     AverageQ1Vals |              11 |\n",
      "|         StdQ1Vals |            5.67 |\n",
      "|         MaxQ1Vals |            35.3 |\n",
      "|         MinQ1Vals |           -11.9 |\n",
      "|     AverageQ2Vals |              11 |\n",
      "|         StdQ2Vals |            5.67 |\n",
      "|         MaxQ2Vals |            35.2 |\n",
      "|         MinQ2Vals |           -13.2 |\n",
      "|            LossPi |           -13.2 |\n",
      "|             LossQ |            1.06 |\n",
      "|              Time |            70.1 |\n",
      "---------------------------------------\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /tmp/experiments/1559656402/simple_save/saved_model.pb\n",
      "---------------------------------------\n",
      "|             Epoch |               3 |\n",
      "|      AverageEpRet |           -45.7 |\n",
      "|          StdEpRet |             335 |\n",
      "|          MaxEpRet |             206 |\n",
      "|          MinEpRet |            -651 |\n",
      "|  AverageTestEpRet |            -501 |\n",
      "|      StdTestEpRet |             103 |\n",
      "|      MaxTestEpRet |            -217 |\n",
      "|      MinTestEpRet |            -590 |\n",
      "|             EpLen |           1e+03 |\n",
      "|         TestEpLen |           1e+03 |\n",
      "| TotalEnvInteracts |         1.5e+04 |\n",
      "|     AverageQ1Vals |            20.6 |\n",
      "|         StdQ1Vals |            7.79 |\n",
      "|         MaxQ1Vals |            61.7 |\n",
      "|         MinQ1Vals |           -1.52 |\n",
      "|     AverageQ2Vals |            20.6 |\n",
      "|         StdQ2Vals |            7.79 |\n",
      "|         MaxQ2Vals |            61.5 |\n",
      "|         MinQ2Vals |           -2.05 |\n",
      "|            LossPi |           -22.3 |\n",
      "|             LossQ |            2.35 |\n",
      "|              Time |             106 |\n",
      "---------------------------------------\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /tmp/experiments/1559656402/simple_save/saved_model.pb\n",
      "---------------------------------------\n",
      "|             Epoch |               4 |\n",
      "|      AverageEpRet |             116 |\n",
      "|          StdEpRet |             387 |\n",
      "|          MaxEpRet |             676 |\n",
      "|          MinEpRet |            -435 |\n",
      "|  AverageTestEpRet |             377 |\n",
      "|      StdTestEpRet |             582 |\n",
      "|      MaxTestEpRet |        1.23e+03 |\n",
      "|      MinTestEpRet |            -535 |\n",
      "|             EpLen |           1e+03 |\n",
      "|         TestEpLen |           1e+03 |\n",
      "| TotalEnvInteracts |           2e+04 |\n",
      "|     AverageQ1Vals |            26.7 |\n",
      "|         StdQ1Vals |              10 |\n",
      "|         MaxQ1Vals |            74.9 |\n",
      "|         MinQ1Vals |           0.269 |\n",
      "|     AverageQ2Vals |            26.7 |\n",
      "|         StdQ2Vals |              10 |\n",
      "|         MaxQ2Vals |            73.6 |\n",
      "|         MinQ2Vals |           0.693 |\n",
      "|            LossPi |           -28.4 |\n",
      "|             LossQ |            3.22 |\n",
      "|              Time |             142 |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import time\n",
    "from spinup.algos.td3 import core\n",
    "from spinup.algos.td3.core import get_vars\n",
    "from spinup.utils.logx import EpochLogger\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for TD3 agents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        return dict(obs1=self.obs1_buf[idxs],\n",
    "                    obs2=self.obs2_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "TD3 (Twin Delayed DDPG)\n",
    "\n",
    "\"\"\"\n",
    "def td3(env_fn, actor_critic=core.mlp_actor_critic, ac_kwargs=dict(), seed=0, \n",
    "        steps_per_epoch=5000, epochs=100, replay_size=int(1e6), gamma=0.99, \n",
    "        polyak=0.995, pi_lr=1e-3, q_lr=1e-3, batch_size=100, start_steps=10000, \n",
    "        act_noise=0.1, target_noise=0.2, noise_clip=0.5, policy_delay=2, \n",
    "        max_ep_len=1000, logger_kwargs=dict(), save_freq=1):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        env_fn : A function which creates a copy of the environment.\n",
    "            The environment must satisfy the OpenAI Gym API.\n",
    "\n",
    "        actor_critic: A function which takes in placeholder symbols \n",
    "            for state, ``x_ph``, and action, ``a_ph``, and returns the main \n",
    "            outputs from the agent's Tensorflow computation graph:\n",
    "\n",
    "            ===========  ================  ======================================\n",
    "            Symbol       Shape             Description\n",
    "            ===========  ================  ======================================\n",
    "            ``pi``       (batch, act_dim)  | Deterministically computes actions\n",
    "                                           | from policy given states.\n",
    "            ``q1``       (batch,)          | Gives one estimate of Q* for \n",
    "                                           | states in ``x_ph`` and actions in\n",
    "                                           | ``a_ph``.\n",
    "            ``q2``       (batch,)          | Gives another estimate of Q* for \n",
    "                                           | states in ``x_ph`` and actions in\n",
    "                                           | ``a_ph``.\n",
    "            ``q1_pi``    (batch,)          | Gives the composition of ``q1`` and \n",
    "                                           | ``pi`` for states in ``x_ph``: \n",
    "                                           | q1(x, pi(x)).\n",
    "            ===========  ================  ======================================\n",
    "\n",
    "        ac_kwargs (dict): Any kwargs appropriate for the actor_critic \n",
    "            function you provided to TD3.\n",
    "\n",
    "        seed (int): Seed for random number generators.\n",
    "\n",
    "        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n",
    "            for the agent and the environment in each epoch.\n",
    "\n",
    "        epochs (int): Number of epochs to run and train agent.\n",
    "\n",
    "        replay_size (int): Maximum length of replay buffer.\n",
    "\n",
    "        gamma (float): Discount factor. (Always between 0 and 1.)\n",
    "\n",
    "        polyak (float): Interpolation factor in polyak averaging for target \n",
    "            networks. Target networks are updated towards main networks \n",
    "            according to:\n",
    "\n",
    "            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow \n",
    "                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta\n",
    "\n",
    "            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually \n",
    "            close to 1.)\n",
    "\n",
    "        pi_lr (float): Learning rate for policy.\n",
    "\n",
    "        q_lr (float): Learning rate for Q-networks.\n",
    "\n",
    "        batch_size (int): Minibatch size for SGD.\n",
    "\n",
    "        start_steps (int): Number of steps for uniform-random action selection,\n",
    "            before running real policy. Helps exploration.\n",
    "\n",
    "        act_noise (float): Stddev for Gaussian exploration noise added to \n",
    "            policy at training time. (At test time, no noise is added.)\n",
    "\n",
    "        target_noise (float): Stddev for smoothing noise added to target \n",
    "            policy.\n",
    "\n",
    "        noise_clip (float): Limit for absolute value of target policy \n",
    "            smoothing noise.\n",
    "\n",
    "        policy_delay (int): Policy will only be updated once every \n",
    "            policy_delay times for each update of the Q-networks.\n",
    "\n",
    "        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n",
    "\n",
    "        logger_kwargs (dict): Keyword args for EpochLogger.\n",
    "\n",
    "        save_freq (int): How often (in terms of gap between epochs) to save\n",
    "            the current policy and value function.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logger = EpochLogger(**logger_kwargs)\n",
    "    logger.save_config(locals())\n",
    "\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env, test_env = env_fn(), env_fn()\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "    act_limit = env.action_space.high[0]\n",
    "\n",
    "    # Share information about action space with policy architecture\n",
    "    ac_kwargs['action_space'] = env.action_space\n",
    "\n",
    "    # Inputs to computation graph\n",
    "    x_ph, a_ph, x2_ph, r_ph, d_ph = core.placeholders(obs_dim, act_dim, obs_dim, None, None)\n",
    "\n",
    "    # Main outputs from computation graph\n",
    "    with tf.variable_scope('main'):\n",
    "        pi, q1, q2, q1_pi = actor_critic(x_ph, a_ph, **ac_kwargs)\n",
    "    \n",
    "    # Target policy network\n",
    "    with tf.variable_scope('target'):\n",
    "        pi_targ, _, _, _  = actor_critic(x2_ph, a_ph, **ac_kwargs)\n",
    "    \n",
    "    # Target Q networks\n",
    "    with tf.variable_scope('target', reuse=True):\n",
    "\n",
    "        # Target policy smoothing, by adding clipped noise to target actions\n",
    "        epsilon = tf.random_normal(tf.shape(pi_targ), stddev=target_noise)\n",
    "        epsilon = tf.clip_by_value(epsilon, -noise_clip, noise_clip)\n",
    "        a2 = pi_targ + epsilon\n",
    "        a2 = tf.clip_by_value(a2, -act_limit, act_limit)\n",
    "\n",
    "        # Target Q-values, using action from target policy\n",
    "        _, q1_targ, q2_targ, _ = actor_critic(x2_ph, a2, **ac_kwargs)\n",
    "\n",
    "    # Experience buffer\n",
    "    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "\n",
    "    # Count variables\n",
    "    var_counts = tuple(core.count_vars(scope) for scope in ['main/pi', 'main/q1', 'main/q2', 'main'])\n",
    "    print('\\nNumber of parameters: \\t pi: %d, \\t q1: %d, \\t q2: %d, \\t total: %d\\n'%var_counts)\n",
    "\n",
    "    # Bellman backup for Q functions, using Clipped Double-Q targets\n",
    "    min_q_targ = tf.minimum(q1_targ, q2_targ)\n",
    "    backup = tf.stop_gradient(r_ph + gamma*(1-d_ph)*min_q_targ)\n",
    "\n",
    "    # TD3 losses\n",
    "    pi_loss = -tf.reduce_mean(q1_pi)\n",
    "    q1_loss = tf.reduce_mean((q1-backup)**2)\n",
    "    q2_loss = tf.reduce_mean((q2-backup)**2)\n",
    "    q_loss = q1_loss + q2_loss\n",
    "    \n",
    "\n",
    "    # Separate train ops for pi, q\n",
    "    pi_optimizer = tf.train.AdamOptimizer(learning_rate=pi_lr)\n",
    "    q_optimizer = tf.train.AdamOptimizer(learning_rate=q_lr)\n",
    "    train_pi_op = pi_optimizer.minimize(pi_loss, var_list=get_vars('main/pi'))\n",
    "    train_q_op = q_optimizer.minimize(q_loss, var_list=get_vars('main/q'))\n",
    "    \n",
    "\n",
    "    # Polyak averaging for target variables\n",
    "    target_update = tf.group([tf.assign(v_targ, polyak*v_targ + (1-polyak)*v_main)\n",
    "                              for v_main, v_targ in zip(get_vars('main'), get_vars('target'))])\n",
    "\n",
    "    # Initializing targets to match main variables\n",
    "    target_init = tf.group([tf.assign(v_targ, v_main)\n",
    "                              for v_main, v_targ in zip(get_vars('main'), get_vars('target'))])\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(target_init)\n",
    "    # log_dir=\"/Users/xieliang/tensorboard/log\"\n",
    "    # tf.summary.FileWriter(log_dir,sess.graph)\n",
    "\n",
    "\n",
    "    # Setup model saving\n",
    "    logger.setup_tf_saver(sess, inputs={'x': x_ph, 'a': a_ph}, outputs={'pi': pi, 'q1': q1, 'q2': q2})\n",
    "\n",
    "    def get_action(o, noise_scale):\n",
    "        a = sess.run(pi, feed_dict={x_ph: o.reshape(1,-1)})[0]\n",
    "        a += noise_scale * np.random.randn(act_dim)\n",
    "        return np.clip(a, -act_limit, act_limit)\n",
    "\n",
    "    def test_agent(n=10):\n",
    "        for j in range(n):\n",
    "            o, r, d, ep_ret, ep_len = test_env.reset(), 0, False, 0, 0\n",
    "            while not(d or (ep_len == max_ep_len)):\n",
    "                # Take deterministic actions at test time (noise_scale=0)\n",
    "                o, r, d, _ = test_env.step(get_action(o, 0))\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "            logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)\n",
    "\n",
    "    start_time = time.time()\n",
    "    o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for t in range(total_steps):\n",
    "\n",
    "        \"\"\"\n",
    "        Until start_steps have elapsed, randomly sample actions\n",
    "        from a uniform distribution for better exploration. Afterwards, \n",
    "        use the learned policy (with some noise, via act_noise). \n",
    "        \"\"\"\n",
    "        if t > start_steps:\n",
    "            a = get_action(o, act_noise)\n",
    "        else:\n",
    "            a = env.action_space.sample()\n",
    "\n",
    "        # Step the env\n",
    "        o2, r, d, _ = env.step(a)\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time\n",
    "        # horizon (that is, when it's an artificial terminal signal\n",
    "        # that isn't based on the agent's state)\n",
    "        d = False if ep_len==max_ep_len else d\n",
    "\n",
    "        # Store experience to replay buffer\n",
    "        replay_buffer.store(o, a, r, o2, d)\n",
    "\n",
    "        # Super critical, easy to overlook step: make sure to update \n",
    "        # most recent observation!\n",
    "        o = o2\n",
    "\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            \"\"\"\n",
    "            Perform all TD3 updates at the end of the trajectory\n",
    "            (in accordance with source code of TD3 published by\n",
    "            original authors).\n",
    "            \"\"\"\n",
    "            for j in range(ep_len):\n",
    "                batch = replay_buffer.sample_batch(batch_size)\n",
    "                feed_dict = {x_ph: batch['obs1'],\n",
    "                             x2_ph: batch['obs2'],\n",
    "                             a_ph: batch['acts'],\n",
    "                             r_ph: batch['rews'],\n",
    "                             d_ph: batch['done']\n",
    "                            }\n",
    "                q_step_ops = [q_loss, q1, q2, train_q_op]\n",
    "                outs = sess.run(q_step_ops, feed_dict)\n",
    "                logger.store(LossQ=outs[0], Q1Vals=outs[1], Q2Vals=outs[2])\n",
    "\n",
    "                if j % policy_delay == 0:\n",
    "                    # Delayed policy update\n",
    "                    outs = sess.run([pi_loss, train_pi_op, target_update], feed_dict)\n",
    "                    logger.store(LossPi=outs[0])\n",
    "\n",
    "            logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "            o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "\n",
    "        # End of epoch wrap-up\n",
    "        if t > 0 and t % steps_per_epoch == 0:\n",
    "            epoch = t // steps_per_epoch\n",
    "\n",
    "            # Save model\n",
    "            if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "                logger.save_state({'env': env}, None)\n",
    "\n",
    "            # Test the performance of the deterministic version of the agent.\n",
    "            test_agent()\n",
    "\n",
    "            # Log info about epoch\n",
    "            logger.log_tabular('Epoch', epoch)\n",
    "            logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "            logger.log_tabular('TestEpRet', with_min_and_max=True)\n",
    "            logger.log_tabular('EpLen', average_only=True)\n",
    "            logger.log_tabular('TestEpLen', average_only=True)\n",
    "            logger.log_tabular('TotalEnvInteracts', t)\n",
    "            logger.log_tabular('Q1Vals', with_min_and_max=True)\n",
    "            logger.log_tabular('Q2Vals', with_min_and_max=True)\n",
    "            logger.log_tabular('LossPi', average_only=True)\n",
    "            logger.log_tabular('LossQ', average_only=True)\n",
    "            logger.log_tabular('Time', time.time()-start_time)\n",
    "            logger.dump_tabular()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "td3(lambda:gym.make('HalfCheetah-v2'),steps_per_epoch=5000,epochs=5)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     import argparse\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--env', type=str, default='HalfCheetah-v2')\n",
    "#     parser.add_argument('--hid', type=int, default=300)\n",
    "#     parser.add_argument('--l', type=int, default=1)\n",
    "#     parser.add_argument('--gamma', type=float, default=0.99)\n",
    "#     parser.add_argument('--seed', '-s', type=int, default=0)\n",
    "#     parser.add_argument('--epochs', type=int, default=50)\n",
    "#     parser.add_argument('--exp_name', type=str, default='td3')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     from spinup.utils.run_utils import setup_logger_kwargs\n",
    "#     logger_kwargs = setup_logger_kwargs(args.exp_name, args.seed)\n",
    "    \n",
    "#     td3(lambda : gym.make(args.env), actor_critic=core.mlp_actor_critic,\n",
    "#         ac_kwargs=dict(hidden_sizes=[args.hid]*args.l),\n",
    "#         gamma=args.gamma, seed=args.seed, epochs=args.epochs,\n",
    "#         logger_kwargs=logger_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl]",
   "language": "python",
   "name": "conda-env-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
