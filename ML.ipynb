{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "## Forward propagation\n",
    "- 什么是独立同分布？  \n",
    "1.独立：前后样本的出现不会互相影响（不要用连续的样本取训练，随机梯度下降可以解决）。  \n",
    "2.同分布：前后样本的出现是随机的，满足同一分布律或分布函数（一般能满足）。  \n",
    "  \n",
    "  \n",
    "- 训练神经网络的样本为什么要独立同分布？    \n",
    "1.连续相关的样本用于训练会导致神经网络对最近训练的样本过拟合，而忘了之前学习到的特征。由于拟合了最近的一些噪声使参数更新的方差（variance）比较大。独立的数据打破样本之间的相关性，通过随机采样互相抵消之间的噪声，加快收敛。  \n",
    "2.用连续相关的样本训练神经网络，算法在连续一段时间内基本朝着同一个方向做gradient descent（拟合噪声），甚至会导致不收敛。 \n",
    "  \n",
    "  \n",
    "- 神经元的本质（不同神经元代表不同的维度）？  \n",
    "  \n",
    "  \n",
    "- 神经网络w和b的作用（简化至二维平面w和b的作用） （本质上是坐标的线性变换，寻找不同的空间以实现分类）？   \n",
    "  \n",
    "  \n",
    "- 激活函数为什么需要sigmoid/relu等非线性激活函数（本质上是非线性变换）？  \n",
    "<img src=\"img/1.png\" width=\"50%\">\n",
    "  \n",
    "  \n",
    "- sigmoid和relu的区别？神经元的输出为什么总是正的？    \n",
    "  \n",
    "  \n",
    "- 如何初始化参数？  \n",
    "1.阈值添加少量噪声打破隐藏对称性。  \n",
    "2.对于relu用一个较小正数初始化偏置项  \n",
    "  \n",
    "  \n",
    "- padding如何操作？    \n",
    "1.（padding=same，自动补全，new_width=width/stride(向上取整)）  \n",
    "2.（padding=valid，不补全，new_width=（width-filter）/stride+1(向下取整)）   \n",
    "  \n",
    "  \n",
    "- 旋转不变性和平移不变性？  \n",
    "  \n",
    "  \n",
    "- dropout如何操作及其作用？   \n",
    "1.[dropout.pdf](http://localhost:8888/files/pdf/dropout.pdf)  \n",
    "2.训练时需要dropout，测试时不需要  \n",
    "3.输出层是线性输出时权值需要乘以drop的比例，输出层是softmax时权值不需要改变。  \n",
    "\n",
    "\n",
    "\n",
    "## Loss function\n",
    "- cross-entropy(最大似然) [cross_entropy.pdf](http://localhost:8888/files/pdf/cross_entropy.pdf) [loss_function.pdf](http://localhost:8888/files/pdf/loss_function.pdf) [visual_information.pdf](http://localhost:8888/files/pdf/visual_information.pdf)  \n",
    "  \n",
    "  \n",
    "- 交叉墒为什么要取对数？     \n",
    "1.乘法变加法，简化计算  \n",
    "2.要求导的话很多常数项可以省去，简化分析  \n",
    "3.大量很小的因子（如概率）直接相乘结果接近0，会产生下溢，计算机直接近似为0，取对数可避免    \n",
    "\n",
    "## Back propagation\n",
    "- 训练为什么需要batch？  \n",
    "1.减少计算开销。  \n",
    "2.最大化地学习到数据集的总体特性。  \n",
    "  \n",
    "  \n",
    "- 优化方法有哪些，如何选择？\n",
    "\n",
    "## 调参经验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "- Session.run()和Tensor.eval()的区别  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL\n",
    "- on-policy和off-policy的区别？      \n",
    "1.在更新Q-table时，on-policy采用greedy选择下一状态的动作，off-policy采用max选择下一状态的动作  \n",
    "2.on-policy通过greedy选择下一状态的动作；off-policy通过greedy选择当前状态的动作。     \n",
    "两者在效果上的区别(实验测试)...  \n",
    "  \n",
    "  \n",
    "- 经验回放为什么只能用于off-policy？ \n",
    "\n",
    "\n",
    "- 为什么要分$\\theta$和$\\theta'$?\n",
    "  \n",
    "  \n",
    "- 经验回放（ER）的作用？    \n",
    "1.获取训练样本；  \n",
    "2.样本可以重复利用进行参数训练；  \n",
    "3.打破数据相关性，减少参数更新的方差，减少不稳定，防止训练不收敛；      \n",
    "4.防止对最近的经验过拟合，过快的忘记一些很少出现但有用的经验（PER）  \n",
    "  \n",
    "  \n",
    "- 重要性采样（IS）  \n",
    "  \n",
    "  \n",
    "- $\\gamma$的意义？  \n",
    "$Q(s,a)=\\mathbb{E}[r_{t+1}+\\gamma r_{t+2}+\\gamma^2r_{t+3}+...|s=s_t,a=a_t]$   \n",
    "在更新Q值时，用来衡量当前reward和将来的reward的重要性，$\\gamma$大,考虑将来的奖励的比重加大  \n",
    "\n",
    "\n",
    "- tabular相比nonlinear approximation function的区别？  \n",
    "当状态空间和动作空间维度较大，tabular的方法学起来相对困难，很多Q值可能学习不到。 AF只需要学习权值  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.6]",
   "language": "python",
   "name": "conda-env-py3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
